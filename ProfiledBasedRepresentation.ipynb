{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "import re\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df  = pd.read_csv('labels_imperium_all.csv',encoding='utf-8',header=None)\n",
    "labels = df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df  = pd.read_csv('imperium_txts_all.csv',encoding='utf-8',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df[0]\n",
    "print(documents[0])\n",
    "# lower case\n",
    "documents = documents.str.lower() \n",
    "# remove punctuation\n",
    "documents = documents.str.replace('[^\\w\\s]','') \n",
    "documents = documents.values.tolist()\n",
    "for i in range(len(documents )):\n",
    "    # tokenize\n",
    "    documents [i] = nltk.word_tokenize(documents [i])\n",
    "    # remove blanck spaces\n",
    "    documents [i] = [re.sub(r'\\W',' ',word) for word in documents [i] ]\n",
    "print(\"Ther are :\",len(documents),\" 0:\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data: Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in documents:\n",
    "    for token in sentence:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "print(\"Ther are :\",len(wordfreq),\" unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordfreq save each unique word and the number of time that his words appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorize function of numpy \n",
    "length_checker = np.vectorize(len) \n",
    "  \n",
    "# Find the length of each element \n",
    "documents_length = length_checker(documents) \n",
    "  \n",
    "# Print the length of each element \n",
    "print(documents_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keys to be deleted from dictionary\n",
    "selectedKeys = list() \n",
    " \n",
    "# Iterate over the dict and put to be deleted keys in the list\n",
    "for (key, value) in wordfreq.items():\n",
    "    if value < 5:\n",
    "        selectedKeys.append(key)\n",
    "    if len(key) < 3:\n",
    "        selectedKeys.append(key)\n",
    "    if len(key) > 30:\n",
    "        selectedKeys.append(key)\n",
    " \n",
    "# Iterate over the list and delete corresponding key from dictionary\n",
    "for key in selectedKeys:\n",
    "    if key in wordfreq:\n",
    "        del wordfreq[key]\n",
    "print(\"Ther are :\",len(wordfreq),\" unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving each unique word index\n",
    "word_idx = {}\n",
    "idx = 0\n",
    "for token in wordfreq:\n",
    "    word_idx[token] =  idx\n",
    "    idx += 1\n",
    "print(word_idx['florida'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in documents:\n",
    "    sent_vec = []\n",
    "    for token in wordfreq:\n",
    "        if token in sentence:\n",
    "            df = sentence.count(token)\n",
    "            sent_vec.append(df)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "print(\"Matrix of dimension: (\",sentence_vectors.shape[0],\",\",sentence_vectors.shape[1],\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the document with more terms\n",
    "maxlen_doc = []\n",
    "for doc in documents:\n",
    "    maxlen_doc.append(len(doc))\n",
    "print(\"Maximum document longitud: \",max(maxlen_doc))  \n",
    "print(max(documents_length))\n",
    "maxlen_doc = (max(documents_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an document / index matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to separete this in sTrain and Stest, we need to be careful to mantein the index in the training fase\n",
    "num_docs = sentence_vectors.shape[0]\n",
    "#maximum = []\n",
    "idx_matriz = np.zeros((num_docs, maxlen_doc)) # idx_matriz represents \"stest\" in Matlab\n",
    "row = 0\n",
    "for doc in documents:\n",
    "    # If tern not in dicc, add '0'\n",
    "    doc_inx = [ word_idx[x] if x in word_idx else 0 for x in doc] \n",
    "    doc_inx.extend(repeat(0, maxlen_doc - len(doc_inx)))  # Here we are padding, but it is NOT necessary\n",
    "    idx_matriz[row] = (doc_inx)\n",
    "    row += 1\n",
    "    # Like this ingnore those words tht are not in dic : doc_inx = [wordfreq[x] for x in doc if x in wordfreq]\n",
    "    #maximum.append(len(doc_inx))\n",
    "print(\"Matrix of dimension: (\",idx_matriz.shape[0],\",\",idx_matriz.shape[1],\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"document_matrix.csv\", sentence_vectors, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df  = pd.read_csv('document_matrix.csv',encoding='utf-8',header=None)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEPARAR SIN PERDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentence_vectors[:6182]\n",
    "X_test  = sentence_vectors[6182:]\n",
    "Y_train = labels[:6182]\n",
    "Y_test  = labels[6182:]\n",
    "S_train = idx_matriz[:6182]\n",
    "S_test  = idx_matriz[6182:]    \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(S_train.shape)\n",
    "print(S_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csa(X_train,Y,X_test):\n",
    "    num_terms = X_train.shape[1]\n",
    "    num_class = 2\n",
    "    dl_train = documents_length [:6182]\n",
    "    dl_test  = documents_length [6182:] \n",
    "    TR = np.zeros((num_terms, num_class))\n",
    "    num_docs = X_train.shape[0]\n",
    "    \n",
    "    # Non - Normalization 0'\n",
    "    for j in range (0,num_class): # j = [0;1]\n",
    "        docs = np.where(Y == j)[0] \n",
    "        for i in range(0,num_terms):\n",
    "            tf_v = X_train[docs,i] # tf the term (v)\n",
    "            TR[i][j] += sum(np.log2(1 + tf_v / dl_train [docs]))\n",
    "\n",
    "    #--print(\"PRB: (\",TR.shape[0],\",\",TR.shape[1],\")\")\n",
    "    TR = np.nan_to_num(TR)\n",
    "    # Normalization 1' Sum all the weights vectors \n",
    "    n1 = np.sum(TR, axis = 0) #Sum by colum\n",
    "    Tik = TR / n1\n",
    "    Tik = np.nan_to_num(Tik)\n",
    "    # Normalization 2' Each componet of tik [ti1,ti2]/ [ti1+ti2;ti1+ti2]\n",
    "    for i in range(0,num_terms):\n",
    "        Tik[i] = Tik[i] / sum(Tik[i]) # Tik[i] = Tik[i] / sum(TR[i])\n",
    "\n",
    "    print(\"Tik: (\",Tik.shape[0],\",\",Tik.shape[1],\")\")\n",
    "    Tik = np.nan_to_num(Tik)\n",
    "    # Document Representation\n",
    "    DR = np.zeros((num_docs, num_class))\n",
    "    for i in range(0,num_docs):\n",
    "        termos = np.where(X_train[i] != 0)[0] # extrac idxs =! 0\n",
    "        a = X_train[i][termos] / dl_train[i]# acces idxs content / len\n",
    "        a = np.expand_dims(a, axis=1)\n",
    "        DR[i] = sum(np.multiply(a,Tik[termos]))\n",
    "        \n",
    "    # Document Test Representation\n",
    "    num_docs = X_test.shape[0]\n",
    "    DRT = np.zeros((num_docs, num_class))\n",
    "    for i in range(0,num_docs):\n",
    "        termos = np.where(X_test[i] != 0)[0] # extrac idxs =! 0\n",
    "        a = X_test[i][termos] / dl_test[i]# acces idxs content / len\n",
    "        a = np.expand_dims(a, axis=1)\n",
    "        DRT[i] = sum(np.multiply(a,Tik[termos]))\n",
    "\n",
    "    return DR,DRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR,DRT = csa(X_train,Y_train,X_test)\n",
    "DR = np.nan_to_num(DR)\n",
    "DRT = np.nan_to_num(DRT)\n",
    "print(DR.shape)\n",
    "print(DRT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in DRT:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PBR: u did this for all X, u shoul do this 4 xtrain and x test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"num_terms = sentence_vectors.shape[1]\n",
    "num_class = 2\n",
    "TR = np.zeros((num_terms, num_class))\n",
    "num_docs = sentence_vectors.shape[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for j in range (0,num_class): # j = [0;1]\n",
    "        docs = np.where(labels == j)[0] \n",
    "        for i in range(0,num_terms):\n",
    "            tf_v = sentence_vectors[docs,i]\n",
    "            TR[i][j] += sum(np.log2(1 + tf_v / documents_length [docs]))\n",
    "\n",
    "print(\"PRB: (\",TR.shape[0],\",\",TR.shape[1],\")\")\n",
    "\n",
    "# Normalization 1' Sum all the weights vectors \n",
    "n1 = np.sum(TR, axis = 0) #Sum by colum\n",
    "Tik = TR / n1\n",
    "\n",
    "# Normalization 2' Each componet of tik [ti1,ti2]/ [ti1+ti2;ti1+ti2]\n",
    "for i in range(0,num_terms):\n",
    "    Tik[i] = Tik[i] / sum(Tik[i]) # Tik[i] = Tik[i] / sum(TR[i])\n",
    "\n",
    "print(\"Tik: (\",Tik.shape[0],\",\",Tik.shape[1],\")\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Representation\n",
    "\"\"\"DR = np.zeros((num_docs, num_class))\n",
    "for i in range(0,num_docs):\n",
    "    termos = np.where(sentence_vectors[i] != 0)[0] # extrac idxs =! 0\n",
    "    a = sentence_vectors[i][termos] / documents_length[i]# acces idxs content / len\n",
    "    a = np.expand_dims(a, axis=1)\n",
    "    DR[i] = sum(np.multiply(a,Tik[termos]))\n",
    "DR.shape\n",
    "# Some of then are empy sentence_vectors[i]  examp 8825 \"Nadie se salva de la regla 34 xd\"\n",
    "# none of the owrds bellow to dicc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"DR: (\",DR.shape[0],\",\",DR.shape[1],\")\")\n",
    "#for i in DR:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train = DR[:6182]\n",
    "X_test  = DR[6182:]\n",
    "Y_train = labels[:6182]\n",
    "Y_test  = labels[6182:]\n",
    "S_train = idx_matriz[:6182]\n",
    "S_test  = idx_matriz[6182:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(S_train.shape)\n",
    "print(S_test.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split( DR, labels, test_size=0.3, random_state=42)\n",
    "#print(X_train.shape)\n",
    "#print(Y_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "#create model\n",
    "num_class = 2\n",
    "model = Sequential()\n",
    "#get number of columns in training data\n",
    "#add model layers\n",
    "model.add(Dense(10, activation='relu', input_shape=(num_class,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(decay=1e-14)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(DR, Y_train, epochs=100, batch_size=10, validation_data=(DRT, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading parcial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retains = [0.0100000000000000,0.0600000000000000,0.110000000000000,0.160000000000000,0.210000000000000,0.260000000000000,0.310000000000000,0.360000000000000,0.410000000000000,0.460000000000000,0.510000000000000,0.560000000000000,0.610000000000000,0.660000000000000,0.710000000000000,0.760000000000000,0.810000000000000,0.860000000000000,0.910000000000000,1]\n",
    "#retains = [0.0100000000000000]\n",
    "for j in range (0,len(retains)):\n",
    "    print(\"---------------------------------------- \",retains[j],\" ---------------------------------------------\")\n",
    "    rXtest = np.zeros((X_test.shape[0], X_test.shape[1]))\n",
    "    for i in range(0,X_test.shape[0]): # iterating each test document\n",
    "        noz = np.where(S_test[i] != 0)[0] # from our idx (ONLY INDXS) document matriz, extract idx != 0 (this step could be avoid if we do not agregate those 0)\n",
    "        ntermssf = round(len(noz)*retains[j]) # We obtain the number of terns with x% of longitud\n",
    "        myox = 1 # minnum Termos = 1\n",
    "        wdix = 0 # \n",
    "        freqtsof = np.zeros(X_test.shape[1])  \n",
    "        # This vector activate the terms present in the Document until that longitud\n",
    "        while myox <= ntermssf:\n",
    "            if S_test[i,wdix] != 0:\n",
    "                freqtsof[int(S_test[i,wdix])] += 1 #If the same word appears more than once , just add one to the position\n",
    "                myox += 1\n",
    "            wdix += 1\n",
    "        rXtest[i] = freqtsof\n",
    "        \n",
    "    DRS,DR_TS = csa(X_train,Y_train,rXtest)\n",
    "    DRS = np.nan_to_num(DRS)\n",
    "    DR_TS = np.nan_to_num(DR_TS)\n",
    "    print(DRS.shape)\n",
    "    print(DR_TS.shape)\n",
    "    his = model.fit(DRS, Y_train, epochs=100, batch_size=10, validation_data=(DR_TS, Y_test))\n",
    "    #Xtestv = rXtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacer funcion las representaciones\n",
    "verificar el pre procesamietno\n",
    "PBR: u did this for all X, u shoul do this 4 xtrain and x test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
