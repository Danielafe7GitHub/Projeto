{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "import re\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df  = pd.read_csv('labels_imperium_all.csv',encoding='utf-8',header=None)\n",
    "labels = df[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0                               \"You fuck your dad.\"\n",
       "1  \"i really don't understand your point.\\xa0 It ...\n",
       "2  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3  \"listen if you dont wanna get married to a man...\n",
       "4  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df  = pd.read_csv('imperium_txts_all.csv',encoding='utf-8',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You fuck your dad.\"\n",
      "Ther are : 8829  0: ['you', 'fuck', 'your', 'dad']\n"
     ]
    }
   ],
   "source": [
    "documents = df[0]\n",
    "print(documents[0])\n",
    "# lower case\n",
    "documents = documents.str.lower() \n",
    "# remove punctuation\n",
    "documents = documents.str.replace('[^\\w\\s]','') \n",
    "documents = documents.values.tolist()\n",
    "for i in range(len(documents )):\n",
    "    # tokenize\n",
    "    documents [i] = nltk.word_tokenize(documents [i])\n",
    "    # remove blanck spaces\n",
    "    documents [i] = [re.sub(r'\\W',' ',word) for word in documents [i] ]\n",
    "print(\"Ther are :\",len(documents),\" 0:\", documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data: Remove puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther are : 30675  unique words\n"
     ]
    }
   ],
   "source": [
    "wordfreq = {}\n",
    "for sentence in documents:\n",
    "    for token in sentence:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "print(\"Ther are :\",len(wordfreq),\" unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordfreq save each unique word and the number of time that his words appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  15  64 ...   8  52 188]\n"
     ]
    }
   ],
   "source": [
    "# Use vectorize function of numpy \n",
    "length_checker = np.vectorize(len) \n",
    "  \n",
    "# Find the length of each element \n",
    "documents_length = length_checker(documents) \n",
    "  \n",
    "# Print the length of each element \n",
    "print(documents_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther are : 4110  unique words\n"
     ]
    }
   ],
   "source": [
    "# List of keys to be deleted from dictionary\n",
    "selectedKeys = list() \n",
    " \n",
    "# Iterate over the dict and put to be deleted keys in the list\n",
    "for (key, value) in wordfreq.items():\n",
    "    if value < 5:\n",
    "        selectedKeys.append(key)\n",
    "    if len(key) < 3:\n",
    "        selectedKeys.append(key)\n",
    "    if len(key) > 30:\n",
    "        selectedKeys.append(key)\n",
    " \n",
    "# Iterate over the list and delete corresponding key from dictionary\n",
    "for key in selectedKeys:\n",
    "    if key in wordfreq:\n",
    "        del wordfreq[key]\n",
    "print(\"Ther are :\",len(wordfreq),\" unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3020\n"
     ]
    }
   ],
   "source": [
    "# Saving each unique word index\n",
    "word_idx = {}\n",
    "idx = 0\n",
    "for token in wordfreq:\n",
    "    word_idx[token] =  idx\n",
    "    idx += 1\n",
    "print(word_idx['florida'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "for sentence in documents:\n",
    "    sent_vec = []\n",
    "    for token in wordfreq:\n",
    "        if token in sentence:\n",
    "            df = sentence.count(token)\n",
    "            sent_vec.append(df)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of dimension: ( 8829 , 4110 )\n"
     ]
    }
   ],
   "source": [
    "sentence_vectors = np.asarray(sentence_vectors)\n",
    "print(\"Matrix of dimension: (\",sentence_vectors.shape[0],\",\",sentence_vectors.shape[1],\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum document longitud:  2299\n",
      "2299\n"
     ]
    }
   ],
   "source": [
    "# Identifying the document with more terms\n",
    "maxlen_doc = []\n",
    "for doc in documents:\n",
    "    maxlen_doc.append(len(doc))\n",
    "print(\"Maximum document longitud: \",max(maxlen_doc))  \n",
    "print(max(documents_length))\n",
    "maxlen_doc = (max(documents_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an document / index matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of dimension: ( 8829 , 2299 )\n"
     ]
    }
   ],
   "source": [
    "# We need to separete this in sTrain and Stest, we need to be careful to mantein the index in the training fase\n",
    "num_docs = sentence_vectors.shape[0]\n",
    "#maximum = []\n",
    "idx_matriz = np.zeros((num_docs, maxlen_doc)) # idx_matriz represents \"stest\" in Matlab\n",
    "row = 0\n",
    "for doc in documents:\n",
    "    # If tern not in dicc, add '0'\n",
    "    doc_inx = [ word_idx[x] if x in word_idx else 0 for x in doc] \n",
    "    doc_inx.extend(repeat(0, maxlen_doc - len(doc_inx)))  # Here we are padding, but it is NOT necessary\n",
    "    idx_matriz[row] = (doc_inx)\n",
    "    row += 1\n",
    "    # Like this ingnore those words tht are not in dic : doc_inx = [wordfreq[x] for x in doc if x in wordfreq]\n",
    "    #maximum.append(len(doc_inx))\n",
    "print(\"Matrix of dimension: (\",idx_matriz.shape[0],\",\",idx_matriz.shape[1],\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"document_matrix.csv\", sentence_vectors, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4100</th>\n",
       "      <th>4101</th>\n",
       "      <th>4102</th>\n",
       "      <th>4103</th>\n",
       "      <th>4104</th>\n",
       "      <th>4105</th>\n",
       "      <th>4106</th>\n",
       "      <th>4107</th>\n",
       "      <th>4108</th>\n",
       "      <th>4109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4100  \\\n",
       "0   1.0   1.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1   1.0   0.0   1.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0  ...   0.0   \n",
       "2   2.0   0.0   2.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  ...   0.0   \n",
       "3   4.0   0.0   3.0   0.0   0.0   2.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "   4101  4102  4103  4104  4105  4106  4107  4108  4109  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 4110 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv('document_matrix.csv',encoding='utf-8',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6182, 4110)\n",
      "(6182,)\n",
      "(2647, 4110)\n",
      "(2647,)\n",
      "(6182, 2299)\n",
      "(2647, 2299)\n"
     ]
    }
   ],
   "source": [
    "X_train = sentence_vectors[:6182]\n",
    "X_test  = sentence_vectors[6182:]\n",
    "Y_train = labels[:6182]\n",
    "Y_test  = labels[6182:]\n",
    "S_train = idx_matriz[:6182]\n",
    "S_test  = idx_matriz[6182:]    \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(S_train.shape)\n",
    "print(S_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csa(X_train,Y,X_test):\n",
    "    num_terms = X_train.shape[1]\n",
    "    num_class = 2\n",
    "    dl_train = documents_length [:6182]\n",
    "    dl_test  = documents_length [6182:] \n",
    "    TR = np.zeros((num_terms, num_class))\n",
    "    num_docs = X_train.shape[0]\n",
    "    \n",
    "    # Non - Normalization 0'\n",
    "    for j in range (0,num_class): # j = [0;1]\n",
    "        docs = np.where(Y == j)[0] \n",
    "        for i in range(0,num_terms):\n",
    "            tf_v = X_train[docs,i] # tf the term (v)\n",
    "            TR[i][j] += sum(np.log2(1 + tf_v / dl_train [docs]))\n",
    "\n",
    "    #--print(\"PRB: (\",TR.shape[0],\",\",TR.shape[1],\")\")\n",
    "    \n",
    "    # Normalization 1' Sum all the weights vectors \n",
    "    n1 = np.sum(TR, axis = 0) #Sum by colum\n",
    "    Tik = TR / n1\n",
    "\n",
    "    # Normalization 2' Each componet of tik [ti1,ti2]/ [ti1+ti2;ti1+ti2]\n",
    "    for i in range(0,num_terms):\n",
    "        Tik[i] = Tik[i] / sum(Tik[i]) # Tik[i] = Tik[i] / sum(TR[i])\n",
    "\n",
    "    #--print(\"Tik: (\",Tik.shape[0],\",\",Tik.shape[1],\")\")\n",
    "    \n",
    "    # Document Representation\n",
    "    DR = np.zeros((num_docs, num_class))\n",
    "    for i in range(0,num_docs):\n",
    "        termos = np.where(X_train[i] != 0)[0] # extrac idxs =! 0\n",
    "        a = X_train[i][termos] / dl_train[i]# acces idxs content / len\n",
    "        a = np.expand_dims(a, axis=1)\n",
    "        DR[i] = sum(np.multiply(a,Tik[termos]))\n",
    "        \n",
    "    # Document Test Representation\n",
    "    num_docs = X_test.shape[0]\n",
    "    TDR = np.zeros((num_docs, num_class))\n",
    "    for i in range(0,num_docs):\n",
    "        termos = np.where(X_test[i] != 0)[0] # extrac idxs =! 0\n",
    "        a = X_test[i][termos] / dl_test[i]# acces idxs content / len\n",
    "        a = np.expand_dims(a, axis=1)\n",
    "        TDR[i] = sum(np.multiply(a,Tik[termos]))\n",
    "\n",
    "    return DR,TDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRB: ( 4110 , 2 )\n",
      "Tik: ( 4110 , 2 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "DR,DRT = csa(X_train,Y_train,X_test)\n",
    "#print(DRS.shape)\n",
    "#print(DR_TS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6182, 2)\n",
      "(2647, 2)\n"
     ]
    }
   ],
   "source": [
    "print(DR.shape)\n",
    "print(DRT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8829"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PBR: u did this for all X, u shoul do this 4 xtrain and x test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_terms = sentence_vectors.shape[1]\\nnum_class = 2\\nTR = np.zeros((num_terms, num_class))\\nnum_docs = sentence_vectors.shape[0]\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"num_terms = sentence_vectors.shape[1]\n",
    "num_class = 2\n",
    "TR = np.zeros((num_terms, num_class))\n",
    "num_docs = sentence_vectors.shape[0]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor j in range (0,num_class): # j = [0;1]\\n        docs = np.where(labels == j)[0] \\n        for i in range(0,num_terms):\\n            tf_v = sentence_vectors[docs,i]\\n            TR[i][j] += sum(np.log2(1 + tf_v / documents_length [docs]))\\n\\nprint(\"PRB: (\",TR.shape[0],\",\",TR.shape[1],\")\")\\n\\n# Normalization 1\\' Sum all the weights vectors \\nn1 = np.sum(TR, axis = 0) #Sum by colum\\nTik = TR / n1\\n\\n# Normalization 2\\' Each componet of tik [ti1,ti2]/ [ti1+ti2;ti1+ti2]\\nfor i in range(0,num_terms):\\n    Tik[i] = Tik[i] / sum(Tik[i]) # Tik[i] = Tik[i] / sum(TR[i])\\n\\nprint(\"Tik: (\",Tik.shape[0],\",\",Tik.shape[1],\")\")\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for j in range (0,num_class): # j = [0;1]\n",
    "        docs = np.where(labels == j)[0] \n",
    "        for i in range(0,num_terms):\n",
    "            tf_v = sentence_vectors[docs,i]\n",
    "            TR[i][j] += sum(np.log2(1 + tf_v / documents_length [docs]))\n",
    "\n",
    "print(\"PRB: (\",TR.shape[0],\",\",TR.shape[1],\")\")\n",
    "\n",
    "# Normalization 1' Sum all the weights vectors \n",
    "n1 = np.sum(TR, axis = 0) #Sum by colum\n",
    "Tik = TR / n1\n",
    "\n",
    "# Normalization 2' Each componet of tik [ti1,ti2]/ [ti1+ti2;ti1+ti2]\n",
    "for i in range(0,num_terms):\n",
    "    Tik[i] = Tik[i] / sum(Tik[i]) # Tik[i] = Tik[i] / sum(TR[i])\n",
    "\n",
    "print(\"Tik: (\",Tik.shape[0],\",\",Tik.shape[1],\")\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DR = np.zeros((num_docs, num_class))\\nfor i in range(0,num_docs):\\n    termos = np.where(sentence_vectors[i] != 0)[0] # extrac idxs =! 0\\n    a = sentence_vectors[i][termos] / documents_length[i]# acces idxs content / len\\n    a = np.expand_dims(a, axis=1)\\n    DR[i] = sum(np.multiply(a,Tik[termos]))\\nDR.shape\\n# Some of then are empy sentence_vectors[i]  examp 8825 \"Nadie se salva de la regla 34 xd\"\\n# none of the owrds bellow to dicc\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Representation\n",
    "\"\"\"DR = np.zeros((num_docs, num_class))\n",
    "for i in range(0,num_docs):\n",
    "    termos = np.where(sentence_vectors[i] != 0)[0] # extrac idxs =! 0\n",
    "    a = sentence_vectors[i][termos] / documents_length[i]# acces idxs content / len\n",
    "    a = np.expand_dims(a, axis=1)\n",
    "    DR[i] = sum(np.multiply(a,Tik[termos]))\n",
    "DR.shape\n",
    "# Some of then are empy sentence_vectors[i]  examp 8825 \"Nadie se salva de la regla 34 xd\"\n",
    "# none of the owrds bellow to dicc\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"DR: (\",DR.shape[0],\",\",DR.shape[1],\")\")\n",
    "#for i in DR:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = DR[:6182]\\nX_test  = DR[6182:]\\nY_train = labels[:6182]\\nY_test  = labels[6182:]\\nS_train = idx_matriz[:6182]\\nS_test  = idx_matriz[6182:]\\nprint(X_train.shape)\\nprint(Y_train.shape)\\nprint(X_test.shape)\\nprint(Y_test.shape)\\nprint(S_train.shape)\\nprint(S_test.shape)\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train = DR[:6182]\n",
    "X_test  = DR[6182:]\n",
    "Y_train = labels[:6182]\n",
    "Y_test  = labels[6182:]\n",
    "S_train = idx_matriz[:6182]\n",
    "S_test  = idx_matriz[6182:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(S_train.shape)\n",
    "print(S_test.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split( DR, labels, test_size=0.3, random_state=42)\n",
    "#print(X_train.shape)\n",
    "#print(Y_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "#create model\n",
    "model = Sequential()\n",
    "#get number of columns in training data\n",
    "#add model layers\n",
    "model.add(Dense(10, activation='relu', input_shape=(num_class,)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.SGD(decay=1e-14)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 6182 samples, validate on 2647 samples\n",
      "Epoch 1/100\n",
      "6182/6182 [==============================] - 2s 398us/step - loss: 0.6357 - acc: 0.6561 - val_loss: nan - val_acc: 0.7261\n",
      "Epoch 2/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.6065 - acc: 0.6563 - val_loss: nan - val_acc: 0.7269\n",
      "Epoch 3/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.5814 - acc: 0.6632 - val_loss: nan - val_acc: 0.7416\n",
      "Epoch 4/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.5533 - acc: 0.6952 - val_loss: nan - val_acc: 0.7469\n",
      "Epoch 5/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.5225 - acc: 0.7310 - val_loss: nan - val_acc: 0.7544\n",
      "Epoch 6/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.4913 - acc: 0.7625 - val_loss: nan - val_acc: 0.7703\n",
      "Epoch 7/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.4615 - acc: 0.7926 - val_loss: nan - val_acc: 0.7752\n",
      "Epoch 8/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.4354 - acc: 0.8077 - val_loss: nan - val_acc: 0.7816\n",
      "Epoch 9/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.4134 - acc: 0.8177 - val_loss: nan - val_acc: 0.7866\n",
      "Epoch 10/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3954 - acc: 0.8247 - val_loss: nan - val_acc: 0.7866\n",
      "Epoch 11/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3811 - acc: 0.8308 - val_loss: nan - val_acc: 0.7900\n",
      "Epoch 12/100\n",
      "6182/6182 [==============================] - 1s 173us/step - loss: 0.3695 - acc: 0.8378 - val_loss: nan - val_acc: 0.7964\n",
      "Epoch 13/100\n",
      "6182/6182 [==============================] - 1s 168us/step - loss: 0.3606 - acc: 0.8402 - val_loss: nan - val_acc: 0.7968\n",
      "Epoch 14/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3536 - acc: 0.8420 - val_loss: nan - val_acc: 0.8002\n",
      "Epoch 15/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3481 - acc: 0.8412 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 16/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3433 - acc: 0.8483 - val_loss: nan - val_acc: 0.7918\n",
      "Epoch 17/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3404 - acc: 0.8454 - val_loss: nan - val_acc: 0.8005\n",
      "Epoch 18/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3373 - acc: 0.8483 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 19/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3351 - acc: 0.8465 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 20/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3333 - acc: 0.8483 - val_loss: nan - val_acc: 0.8009\n",
      "Epoch 21/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3324 - acc: 0.8486 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 22/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3310 - acc: 0.8492 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 23/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3305 - acc: 0.8505 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 24/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3296 - acc: 0.8494 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 25/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3286 - acc: 0.8499 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 26/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3280 - acc: 0.8522 - val_loss: nan - val_acc: 0.8002\n",
      "Epoch 27/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3281 - acc: 0.8518 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 28/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3267 - acc: 0.8517 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 29/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3271 - acc: 0.8489 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 30/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3271 - acc: 0.8525 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 31/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3264 - acc: 0.8496 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 32/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3263 - acc: 0.8520 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 33/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3261 - acc: 0.8522 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 34/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3255 - acc: 0.8522 - val_loss: nan - val_acc: 0.7990\n",
      "Epoch 35/100\n",
      "6182/6182 [==============================] - 1s 169us/step - loss: 0.3254 - acc: 0.8517 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 36/100\n",
      "6182/6182 [==============================] - 1s 177us/step - loss: 0.3255 - acc: 0.8525 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 37/100\n",
      "6182/6182 [==============================] - 1s 169us/step - loss: 0.3249 - acc: 0.8500 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 38/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3249 - acc: 0.8517 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 39/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3241 - acc: 0.8520 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 40/100\n",
      "6182/6182 [==============================] - 1s 173us/step - loss: 0.3244 - acc: 0.8522 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 41/100\n",
      "6182/6182 [==============================] - 1s 170us/step - loss: 0.3241 - acc: 0.8517 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 42/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3236 - acc: 0.8531 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 43/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3232 - acc: 0.8538 - val_loss: nan - val_acc: 0.8009\n",
      "Epoch 44/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3233 - acc: 0.8544 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 45/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3227 - acc: 0.8555 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 46/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3228 - acc: 0.8528 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 47/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3223 - acc: 0.8551 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 48/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3220 - acc: 0.8525 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 49/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3216 - acc: 0.8528 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 50/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3214 - acc: 0.8534 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 51/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3212 - acc: 0.8531 - val_loss: nan - val_acc: 0.7990\n",
      "Epoch 52/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3207 - acc: 0.8539 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 53/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3205 - acc: 0.8538 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 54/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3201 - acc: 0.8541 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 55/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3200 - acc: 0.8549 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 56/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3201 - acc: 0.8536 - val_loss: nan - val_acc: 0.7983\n",
      "Epoch 57/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3194 - acc: 0.8518 - val_loss: nan - val_acc: 0.7968\n",
      "Epoch 58/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3190 - acc: 0.8554 - val_loss: nan - val_acc: 0.7990\n",
      "Epoch 59/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3189 - acc: 0.8530 - val_loss: nan - val_acc: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3187 - acc: 0.8530 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 61/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3182 - acc: 0.8539 - val_loss: nan - val_acc: 0.7941\n",
      "Epoch 62/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3189 - acc: 0.8526 - val_loss: nan - val_acc: 0.7990\n",
      "Epoch 63/100\n",
      "6182/6182 [==============================] - 1s 163us/step - loss: 0.3178 - acc: 0.8509 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 64/100\n",
      "6182/6182 [==============================] - 1s 165us/step - loss: 0.3182 - acc: 0.8551 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 65/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3178 - acc: 0.8536 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 66/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3177 - acc: 0.8528 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 67/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3182 - acc: 0.8541 - val_loss: nan - val_acc: 0.7998\n",
      "Epoch 68/100\n",
      "6182/6182 [==============================] - 1s 164us/step - loss: 0.3177 - acc: 0.8512 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 69/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3171 - acc: 0.8531 - val_loss: nan - val_acc: 0.7930\n",
      "Epoch 70/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3173 - acc: 0.8536 - val_loss: nan - val_acc: 0.7949\n",
      "Epoch 71/100\n",
      "6182/6182 [==============================] - 1s 165us/step - loss: 0.3169 - acc: 0.8544 - val_loss: nan - val_acc: 0.7926\n",
      "Epoch 72/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3170 - acc: 0.8533 - val_loss: nan - val_acc: 0.7971\n",
      "Epoch 73/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3168 - acc: 0.8533 - val_loss: nan - val_acc: 0.7952\n",
      "Epoch 74/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3168 - acc: 0.8546 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 75/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3157 - acc: 0.8543 - val_loss: nan - val_acc: 0.7945\n",
      "Epoch 76/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3164 - acc: 0.8552 - val_loss: nan - val_acc: 0.7934\n",
      "Epoch 77/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3160 - acc: 0.8546 - val_loss: nan - val_acc: 0.7998\n",
      "Epoch 78/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3152 - acc: 0.8549 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 79/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3162 - acc: 0.8551 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 80/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3159 - acc: 0.8538 - val_loss: nan - val_acc: 0.8002\n",
      "Epoch 81/100\n",
      "6182/6182 [==============================] - 1s 163us/step - loss: 0.3159 - acc: 0.8541 - val_loss: nan - val_acc: 0.7949\n",
      "Epoch 82/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3156 - acc: 0.8533 - val_loss: nan - val_acc: 0.7994\n",
      "Epoch 83/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3157 - acc: 0.8554 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 84/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3147 - acc: 0.8534 - val_loss: nan - val_acc: 0.7956\n",
      "Epoch 85/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3153 - acc: 0.8549 - val_loss: nan - val_acc: 0.7952\n",
      "Epoch 86/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3151 - acc: 0.8544 - val_loss: nan - val_acc: 0.7960\n",
      "Epoch 87/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3146 - acc: 0.8554 - val_loss: nan - val_acc: 0.7990\n",
      "Epoch 88/100\n",
      "6182/6182 [==============================] - 1s 164us/step - loss: 0.3153 - acc: 0.8547 - val_loss: nan - val_acc: 0.7979\n",
      "Epoch 89/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3146 - acc: 0.8551 - val_loss: nan - val_acc: 0.7952\n",
      "Epoch 90/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3148 - acc: 0.8570 - val_loss: nan - val_acc: 0.7952\n",
      "Epoch 91/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3147 - acc: 0.8544 - val_loss: nan - val_acc: 0.7952\n",
      "Epoch 92/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3142 - acc: 0.8551 - val_loss: nan - val_acc: 0.8020\n",
      "Epoch 93/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3141 - acc: 0.8555 - val_loss: nan - val_acc: 0.7986\n",
      "Epoch 94/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3142 - acc: 0.8544 - val_loss: nan - val_acc: 0.7949\n",
      "Epoch 95/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3140 - acc: 0.8554 - val_loss: nan - val_acc: 0.7998\n",
      "Epoch 96/100\n",
      "6182/6182 [==============================] - 1s 168us/step - loss: 0.3140 - acc: 0.8546 - val_loss: nan - val_acc: 0.7956\n",
      "Epoch 97/100\n",
      "6182/6182 [==============================] - 1s 165us/step - loss: 0.3142 - acc: 0.8555 - val_loss: nan - val_acc: 0.7975\n",
      "Epoch 98/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3136 - acc: 0.8549 - val_loss: nan - val_acc: 0.8002\n",
      "Epoch 99/100\n",
      "6182/6182 [==============================] - 1s 163us/step - loss: 0.3139 - acc: 0.8557 - val_loss: nan - val_acc: 0.7971\n",
      "Epoch 100/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3138 - acc: 0.8549 - val_loss: nan - val_acc: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2935fd4b488>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(DR, Y_train, epochs=100, batch_size=10, validation_data=(DRT, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading parcial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------  0.01  ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DanielaFe7\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6182, 2)\n",
      "(2647, 2)\n",
      "Train on 6182 samples, validate on 2647 samples\n",
      "Epoch 1/100\n",
      "6182/6182 [==============================] - 1s 160us/step - loss: 0.3092 - acc: 0.8586 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 2/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3100 - acc: 0.8570 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 3/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3094 - acc: 0.8585 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 4/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3102 - acc: 0.8596 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 5/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3099 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 6/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3098 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 7/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3095 - acc: 0.8581 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 8/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3094 - acc: 0.8602 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 9/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3099 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 10/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3095 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 11/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3093 - acc: 0.8583 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 12/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3095 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 13/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3099 - acc: 0.8586 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 14/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3097 - acc: 0.8586 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 15/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3092 - acc: 0.8585 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 16/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3101 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 17/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3094 - acc: 0.8589 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 18/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3093 - acc: 0.8602 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 19/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3094 - acc: 0.8580 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 20/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3095 - acc: 0.8591 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 21/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3097 - acc: 0.8573 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 22/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3092 - acc: 0.8596 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 23/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3097 - acc: 0.8604 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 24/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3095 - acc: 0.8594 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 25/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3095 - acc: 0.8614 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 26/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3092 - acc: 0.8615 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 27/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3096 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 28/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3093 - acc: 0.8609 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 29/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3096 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 30/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3093 - acc: 0.8607 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 31/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3091 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 32/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3096 - acc: 0.8604 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 33/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3089 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 34/100\n",
      "6182/6182 [==============================] - 1s 154us/step - loss: 0.3099 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 35/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3099 - acc: 0.8585 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 36/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3095 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 37/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3099 - acc: 0.8589 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 38/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3099 - acc: 0.8589 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 39/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3096 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 40/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3095 - acc: 0.8594 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 41/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3094 - acc: 0.8596 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 42/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3096 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 43/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3096 - acc: 0.8604 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 44/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3088 - acc: 0.8588 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 45/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3098 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 46/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3092 - acc: 0.8564 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 47/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3096 - acc: 0.8607 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 48/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3087 - acc: 0.8588 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 49/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3094 - acc: 0.8606 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 50/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3094 - acc: 0.8583 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 51/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3088 - acc: 0.8610 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 52/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3091 - acc: 0.8614 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 53/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3098 - acc: 0.8606 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 54/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3094 - acc: 0.8602 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 55/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3096 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 56/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3088 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 57/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3097 - acc: 0.8567 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 58/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3088 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 59/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3092 - acc: 0.8586 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 60/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3093 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3095 - acc: 0.8580 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 62/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3097 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 63/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3089 - acc: 0.8606 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 64/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3088 - acc: 0.8606 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 65/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3093 - acc: 0.8601 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 66/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3097 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 67/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3094 - acc: 0.8598 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 68/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3092 - acc: 0.8601 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 69/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3095 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 70/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3089 - acc: 0.8609 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 71/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3089 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 72/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3091 - acc: 0.8614 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 73/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3092 - acc: 0.8610 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 74/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3090 - acc: 0.8596 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 75/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3094 - acc: 0.8601 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 76/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3095 - acc: 0.8591 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 77/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3088 - acc: 0.8602 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 78/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3093 - acc: 0.8627 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 79/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3090 - acc: 0.8583 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 80/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3092 - acc: 0.8606 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 81/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3091 - acc: 0.8578 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 82/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3093 - acc: 0.8589 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 83/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3091 - acc: 0.8589 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 84/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3088 - acc: 0.8612 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 85/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3092 - acc: 0.8612 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 86/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3095 - acc: 0.8610 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 87/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3091 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 88/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3093 - acc: 0.8612 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 89/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3090 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 90/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3093 - acc: 0.8604 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 91/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3092 - acc: 0.8614 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 92/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3091 - acc: 0.8610 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 93/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3093 - acc: 0.8615 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 94/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3094 - acc: 0.8604 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 95/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3093 - acc: 0.8596 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 96/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3093 - acc: 0.8593 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 97/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3090 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 98/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3095 - acc: 0.8601 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 99/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3092 - acc: 0.8594 - val_loss: nan - val_acc: 0.7378\n",
      "Epoch 100/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3085 - acc: 0.8599 - val_loss: nan - val_acc: 0.7378\n",
      "----------------------------------------  0.06  ---------------------------------------------\n",
      "(6182, 2)\n",
      "(2647, 2)\n",
      "Train on 6182 samples, validate on 2647 samples\n",
      "Epoch 1/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3092 - acc: 0.8586 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 2/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3090 - acc: 0.8602 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 3/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3093 - acc: 0.8596 - val_loss: nan - val_acc: 0.7348\n",
      "Epoch 4/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3094 - acc: 0.8607 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 5/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3087 - acc: 0.8585 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 6/100\n",
      "6182/6182 [==============================] - 1s 163us/step - loss: 0.3088 - acc: 0.8601 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 7/100\n",
      "6182/6182 [==============================] - 1s 166us/step - loss: 0.3089 - acc: 0.8617 - val_loss: nan - val_acc: 0.7348\n",
      "Epoch 8/100\n",
      "6182/6182 [==============================] - 1s 154us/step - loss: 0.3090 - acc: 0.8615 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 9/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3085 - acc: 0.8610 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 10/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3091 - acc: 0.8615 - val_loss: nan - val_acc: 0.7348\n",
      "Epoch 11/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3092 - acc: 0.8596 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 12/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3088 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 13/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3093 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 14/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3089 - acc: 0.8610 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 15/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3085 - acc: 0.8610 - val_loss: nan - val_acc: 0.7348\n",
      "Epoch 16/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3087 - acc: 0.8615 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 17/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3085 - acc: 0.8625 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 18/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3087 - acc: 0.8610 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 19/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3084 - acc: 0.8599 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3088 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 21/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3090 - acc: 0.8610 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 22/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3087 - acc: 0.8607 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 23/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3089 - acc: 0.8630 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 24/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3092 - acc: 0.8604 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 25/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3087 - acc: 0.8589 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 26/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3088 - acc: 0.8601 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 27/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3085 - acc: 0.8591 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 28/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3090 - acc: 0.8606 - val_loss: nan - val_acc: 0.7348\n",
      "Epoch 29/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3085 - acc: 0.8599 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 30/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3088 - acc: 0.8609 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 31/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3093 - acc: 0.8604 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 32/100\n",
      "6182/6182 [==============================] - 1s 154us/step - loss: 0.3090 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 33/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3092 - acc: 0.8614 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 34/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3087 - acc: 0.8598 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 35/100\n",
      "6182/6182 [==============================] - 1s 143us/step - loss: 0.3089 - acc: 0.8604 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 36/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3091 - acc: 0.8612 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 37/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3086 - acc: 0.8623 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 38/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3090 - acc: 0.8607 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 39/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3091 - acc: 0.8610 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 40/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3092 - acc: 0.8610 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 41/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3091 - acc: 0.8607 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 42/100\n",
      "6182/6182 [==============================] - 1s 155us/step - loss: 0.3087 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 43/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3087 - acc: 0.8604 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 44/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3085 - acc: 0.8612 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 45/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3088 - acc: 0.8604 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 46/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3087 - acc: 0.8607 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 47/100\n",
      "6182/6182 [==============================] - 1s 159us/step - loss: 0.3091 - acc: 0.8612 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 48/100\n",
      "6182/6182 [==============================] - 1s 166us/step - loss: 0.3086 - acc: 0.8610 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 49/100\n",
      "6182/6182 [==============================] - 1s 165us/step - loss: 0.3090 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 50/100\n",
      "6182/6182 [==============================] - 1s 158us/step - loss: 0.3084 - acc: 0.8614 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 51/100\n",
      "6182/6182 [==============================] - 1s 161us/step - loss: 0.3090 - acc: 0.8607 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 52/100\n",
      "6182/6182 [==============================] - 1s 155us/step - loss: 0.3087 - acc: 0.8606 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 53/100\n",
      "6182/6182 [==============================] - 1s 153us/step - loss: 0.3086 - acc: 0.8614 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 54/100\n",
      "6182/6182 [==============================] - 1s 165us/step - loss: 0.3089 - acc: 0.8619 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 55/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3089 - acc: 0.8609 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 56/100\n",
      "6182/6182 [==============================] - 1s 166us/step - loss: 0.3088 - acc: 0.8628 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 57/100\n",
      "6182/6182 [==============================] - 1s 156us/step - loss: 0.3089 - acc: 0.8614 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 58/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3086 - acc: 0.8601 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 59/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3089 - acc: 0.8619 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 60/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3090 - acc: 0.8620 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 61/100\n",
      "6182/6182 [==============================] - 1s 162us/step - loss: 0.3087 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 62/100\n",
      "6182/6182 [==============================] - 1s 163us/step - loss: 0.3083 - acc: 0.8601 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 63/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3075 - acc: 0.8623 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 64/100\n",
      "6182/6182 [==============================] - 1s 142us/step - loss: 0.3084 - acc: 0.8625 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 65/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3089 - acc: 0.8627 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 66/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3088 - acc: 0.8632 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 67/100\n",
      "6182/6182 [==============================] - 1s 155us/step - loss: 0.3086 - acc: 0.8598 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 68/100\n",
      "6182/6182 [==============================] - 1s 171us/step - loss: 0.3089 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 69/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3090 - acc: 0.8607 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 70/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3086 - acc: 0.8604 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 71/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3085 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 72/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3085 - acc: 0.8604 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 73/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3084 - acc: 0.8612 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 74/100\n",
      "6182/6182 [==============================] - 1s 153us/step - loss: 0.3090 - acc: 0.8610 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 75/100\n",
      "6182/6182 [==============================] - 1s 164us/step - loss: 0.3085 - acc: 0.8619 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 76/100\n",
      "6182/6182 [==============================] - 1s 171us/step - loss: 0.3087 - acc: 0.8585 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 77/100\n",
      "6182/6182 [==============================] - 1s 167us/step - loss: 0.3086 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 78/100\n",
      "6182/6182 [==============================] - 1s 164us/step - loss: 0.3091 - acc: 0.8599 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 79/100\n",
      "6182/6182 [==============================] - 1s 167us/step - loss: 0.3086 - acc: 0.8614 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 80/100\n",
      "6182/6182 [==============================] - 1s 175us/step - loss: 0.3087 - acc: 0.8630 - val_loss: nan - val_acc: 0.7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3090 - acc: 0.8627 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 82/100\n",
      "6182/6182 [==============================] - ETA: 0s - loss: 0.3071 - acc: 0.863 - 1s 154us/step - loss: 0.3085 - acc: 0.8628 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 83/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3089 - acc: 0.8604 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 84/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3088 - acc: 0.8610 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 85/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3089 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 86/100\n",
      "6182/6182 [==============================] - 1s 154us/step - loss: 0.3087 - acc: 0.8623 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 87/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3088 - acc: 0.8586 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 88/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3084 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 89/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3085 - acc: 0.8617 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 90/100\n",
      "6182/6182 [==============================] - 1s 149us/step - loss: 0.3088 - acc: 0.8596 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 91/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3088 - acc: 0.8619 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 92/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3081 - acc: 0.8627 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 93/100\n",
      "6182/6182 [==============================] - 1s 146us/step - loss: 0.3087 - acc: 0.8594 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 94/100\n",
      "6182/6182 [==============================] - 1s 151us/step - loss: 0.3084 - acc: 0.8615 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 95/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3086 - acc: 0.8591 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 96/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3086 - acc: 0.8612 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 97/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3086 - acc: 0.8609 - val_loss: nan - val_acc: 0.7337\n",
      "Epoch 98/100\n",
      "6182/6182 [==============================] - 1s 155us/step - loss: 0.3086 - acc: 0.8607 - val_loss: nan - val_acc: 0.7340\n",
      "Epoch 99/100\n",
      "6182/6182 [==============================] - 1s 155us/step - loss: 0.3086 - acc: 0.8630 - val_loss: nan - val_acc: 0.7344\n",
      "Epoch 100/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3082 - acc: 0.8612 - val_loss: nan - val_acc: 0.7337\n",
      "----------------------------------------  0.11  ---------------------------------------------\n",
      "(6182, 2)\n",
      "(2647, 2)\n",
      "Train on 6182 samples, validate on 2647 samples\n",
      "Epoch 1/100\n",
      "6182/6182 [==============================] - 1s 157us/step - loss: 0.3079 - acc: 0.8617 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 2/100\n",
      "6182/6182 [==============================] - 1s 156us/step - loss: 0.3085 - acc: 0.8622 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 3/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3090 - acc: 0.8589 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 4/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3077 - acc: 0.8617 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 5/100\n",
      "6182/6182 [==============================] - 1s 145us/step - loss: 0.3090 - acc: 0.8585 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 6/100\n",
      "6182/6182 [==============================] - 1s 147us/step - loss: 0.3089 - acc: 0.8598 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 7/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3090 - acc: 0.8614 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 8/100\n",
      "6182/6182 [==============================] - 1s 143us/step - loss: 0.3082 - acc: 0.8604 - val_loss: nan - val_acc: 0.7310\n",
      "Epoch 9/100\n",
      "6182/6182 [==============================] - 1s 141us/step - loss: 0.3089 - acc: 0.8614 - val_loss: nan - val_acc: 0.7306\n",
      "Epoch 10/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3090 - acc: 0.8614 - val_loss: nan - val_acc: 0.7306\n",
      "Epoch 11/100\n",
      "6182/6182 [==============================] - 1s 143us/step - loss: 0.3081 - acc: 0.8615 - val_loss: nan - val_acc: 0.7310\n",
      "Epoch 12/100\n",
      "6182/6182 [==============================] - 1s 143us/step - loss: 0.3089 - acc: 0.8612 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 13/100\n",
      "6182/6182 [==============================] - 1s 142us/step - loss: 0.3087 - acc: 0.8622 - val_loss: nan - val_acc: 0.7310\n",
      "Epoch 14/100\n",
      "6182/6182 [==============================] - 1s 141us/step - loss: 0.3088 - acc: 0.8596 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 15/100\n",
      "6182/6182 [==============================] - 1s 143us/step - loss: 0.3087 - acc: 0.8635 - val_loss: nan - val_acc: 0.7310\n",
      "Epoch 16/100\n",
      "6182/6182 [==============================] - 1s 141us/step - loss: 0.3084 - acc: 0.8623 - val_loss: nan - val_acc: 0.7306\n",
      "Epoch 17/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3087 - acc: 0.8614 - val_loss: nan - val_acc: 0.7310\n",
      "Epoch 18/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3086 - acc: 0.8630 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 19/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3089 - acc: 0.8599 - val_loss: nan - val_acc: 0.7306\n",
      "Epoch 20/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3082 - acc: 0.8619 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 21/100\n",
      "6182/6182 [==============================] - 1s 144us/step - loss: 0.3087 - acc: 0.8627 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 22/100\n",
      "6182/6182 [==============================] - 1s 148us/step - loss: 0.3089 - acc: 0.8610 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 23/100\n",
      "6182/6182 [==============================] - 1s 154us/step - loss: 0.3085 - acc: 0.8606 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 24/100\n",
      "6182/6182 [==============================] - 1s 150us/step - loss: 0.3085 - acc: 0.8619 - val_loss: nan - val_acc: 0.7306\n",
      "Epoch 25/100\n",
      "6182/6182 [==============================] - 1s 152us/step - loss: 0.3090 - acc: 0.8619 - val_loss: nan - val_acc: 0.7314\n",
      "Epoch 26/100\n",
      "5250/6182 [========================>.....] - ETA: 0s - loss: 0.3035 - acc: 0.8636"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-4b7ae85b2066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDR_TS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#Xtestv = rXtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RendesNeurais\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retains = [0.0100000000000000,0.0600000000000000,0.110000000000000,0.160000000000000,0.210000000000000,0.260000000000000,0.310000000000000,0.360000000000000,0.410000000000000,0.460000000000000,0.510000000000000,0.560000000000000,0.610000000000000,0.660000000000000,0.710000000000000,0.760000000000000,0.810000000000000,0.860000000000000,0.910000000000000,1]\n",
    "#retains = [0.910000000000000]\n",
    "\n",
    "for j in range (0,len(retains)):\n",
    "    print(\"---------------------------------------- \",retains[j],\" ---------------------------------------------\")\n",
    "    rXtest = np.zeros((X_test.shape[0], num_terms))\n",
    "    for i in range(0,X_test.shape[0]): # iterating each test document\n",
    "        noz = np.where(S_test[i] != 0)[0] # from our idx (ONLY INDXS) document matriz, extract idx != 0 (this step could be avoid if we do not agregate those 0)\n",
    "        ntermssf = round(len(noz)*retains[j]) # We obtain the number of terns with x% of longitud\n",
    "        myox = 1 # minnum Termos = 1\n",
    "        wdix = 0 # \n",
    "        freqtsof = np.zeros(num_terms)  \n",
    "        # This vector activate the terms present in the Document until that longitud\n",
    "        while myox <= ntermssf:\n",
    "            if S_test[i,wdix] != 0:\n",
    "                freqtsof[int(S_test[i,wdix])] += 1 #If the same word appears more than once , just add one to the position\n",
    "                myox += 1\n",
    "            wdix += 1\n",
    "        rXtest[i] = freqtsof\n",
    "        \n",
    "    DR,DRT = csa(X_train,Y_train,rXtest)\n",
    "    print(DRS.shape)\n",
    "    print(DR_TS.shape)\n",
    "    model.fit(DR, Y_train, epochs=100, batch_size=10, validation_data=(DRT, Y_test))\n",
    "    #Xtestv = rXtest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FIX OS NAN VALL LOSS BECAUSE THE NAN DO LIKE CODE THEY ELIMINE THEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hacer funcion las representaciones\n",
    "verificar el pre procesamietno\n",
    "PBR: u did this for all X, u shoul do this 4 xtrain and x test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
